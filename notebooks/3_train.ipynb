{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "import numpy as np\n",
    "from codecs import open\n",
    "from joblib import dump\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.word_vector import sentence_vectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import abspath, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_sent(sentence):\n",
    "    sents = sentence.split(' ')\n",
    "    sents = list(dict.fromkeys(sents)) # remove duplicated by keep the order of keys\n",
    "    return ''.join(sents)\n",
    "def remove_stopwords(sentence):\n",
    "    words = list(filter(lambda word: not word in thai_stopwords(), word_tokenize(sentence)))\n",
    "    return ''.join(words)\n",
    "def clean_text(sentence):\n",
    "    sentence = remove_duplicated_sent(sentence)\n",
    "    #sentence = remove_stopwords(sentence)\n",
    "    return sentence\n",
    "def load_dataset():\n",
    "    dataset = []\n",
    "    with open(abspath(join('..', 'dataset', 'useful-tweets.json')), 'r', encoding='utf-8-sig') as f:\n",
    "        positive_tweets_json = json.load(f)\n",
    "    with open(abspath(join('..', 'dataset', 'useless-tweets.json')), 'r', encoding='utf-8-sig') as f:\n",
    "        negative_tweets_json = json.load(f)\n",
    "    for index in range(max(len(positive_tweets_json), len(negative_tweets_json))):\n",
    "        if index < len(positive_tweets_json):\n",
    "            # clean text\n",
    "            text = clean_text(positive_tweets_json[index]['text'])\n",
    "            # convert a Thai sentence into vector shape (1, 300)\n",
    "            vect = sentence_vectorizer(text)\n",
    "            dataset.append((vect, 1))\n",
    "        if index < len(negative_tweets_json):\n",
    "            # clean text\n",
    "            text = clean_text(negative_tweets_json[index]['text'])\n",
    "            # convert a Thai sentence into vector shape (1, 300)\n",
    "            vect = sentence_vectorizer(text)\n",
    "            dataset.append((vect, 0))\n",
    "    random.shuffle(dataset)\n",
    "    return np.array([X[0] for X in dataset]), np.array([y[1] for y in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (1006, 300)\n",
      "X_test shape : (432, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_train, X_test = X_train.reshape(-1, X_train.shape[2]), X_test.reshape(-1, X_test.shape[2])\n",
    "print(f'X_train shape : {X_train.shape}\\nX_test shape : {X_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy :  92.5925925925926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\Coding\\\\Python\\\\BeneficialTweetCOVID-19\\\\model\\\\svm_classifier.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support vector machine classifier\n",
    "classifier = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "classifier.fit(X_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(\"SVM Accuracy : \", accuracy_score(y_pred, y_test)*100)\n",
    "dump(classifier, abspath(join('..', 'model', 'svm_classifier.joblib'))) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy :  93.05555555555556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\Coding\\\\Python\\\\BeneficialTweetCOVID-19\\\\model\\\\rf_classifier.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=1, criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred2 = clf.predict(X_test)\n",
    "print(\"RF Accuracy : \", accuracy_score(y_pred2, y_test)*100)\n",
    "dump(classifier, abspath(join('..', 'model', 'rf_classifier.joblib'))) # save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('useful-tweets': conda)",
   "language": "python",
   "name": "python38264bitusefultweetscondad980e3430077457f81c805f459b4d055"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
